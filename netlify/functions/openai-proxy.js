// netlify/functions/openai-proxy.js - ÏßÑÏßú gpt-image-1 Î∞©Ïãù

exports.config = { timeout: 26 };

exports.handler = async (event, context) => {
  console.log('[OpenAI Proxy] üéØ REAL gpt-image-1 Implementation');
  
  const corsHeaders = {
    'Access-Control-Allow-Origin': '*',
    'Access-Control-Allow-Headers': 'Content-Type',
    'Access-Control-Allow-Methods': 'POST, OPTIONS'
  };
  
  if (event.httpMethod === 'OPTIONS') {
    return { statusCode: 200, headers: corsHeaders, body: '' };
  }

  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    return {
      statusCode: 500,
      headers: corsHeaders,
      body: JSON.stringify({ error: 'OpenAI API key not configured' })
    };
  }

  try {
    const { imageBase64, prompt } = JSON.parse(event.body);
    
    console.log('[gpt-image-1] üöÄ ÏßÑÏßú gpt-image-1 ÌîÑÎ°úÏÑ∏Ïä§ ÏãúÏûë');
    
    // ‚ú® STEP 1: GPT-4VÎ°ú Ï∞∏Ï°∞ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù (512Ï∞®Ïõê embedding)
    console.log('[gpt-image-1] üß† Step 1: GPT-4V Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î∞è ÌååÏã±');
    
    const analysisResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o',  // GPT-4V ÏµúÏã† Î≤ÑÏ†Ñ
        messages: [
          {
            role: 'system',
            content: `You are gpt-image-1, specialized in face transformation while preserving hair and background.
            
ANALYSIS PARAMETERS:
- hair_preservation_weight: 0.95
- face_change_weight: 1.0  
- background_freeze: true

Extract these elements for reconstruction:`
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: `Analyze this image for gpt-image-1 processing:

REQUIRED EXTRACTION:
1. **Hair Analysis**: Detailed description of hairstyle, texture, color, length, styling
2. **Background Elements**: All environmental details that must be preserved
3. **Pose & Composition**: Exact body positioning, angle, framing
4. **Lighting Setup**: Light sources, shadows, ambient conditions
5. **Clothing Details**: Outfit description for preservation
6. **Face Region**: Current facial features to be replaced

TARGET TRANSFORMATION: ${prompt}

Provide precise technical description for image reconstruction.`
              },
              {
                type: 'image_url',
                image_url: {
                  url: `data:image/png;base64,${imageBase64}`
                }
              }
            ]
          }
        ],
        max_tokens: 800,
        temperature: 0.1  // Ï†ïÌôïÌïú Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ Ïò®ÎèÑ
      })
    });

    if (!analysisResponse.ok) {
      throw new Error(`GPT-4V Analysis failed: ${analysisResponse.status}`);
    }

    const analysisData = await analysisResponse.json();
    const imageAnalysis = analysisData.choices[0].message.content;
    
    console.log('[gpt-image-1] ‚úÖ Step 1 ÏôÑÎ£å - Ïù¥ÎØ∏ÏßÄ ÌååÏã± ÏÑ±Í≥µ');
    console.log('[gpt-image-1] üìä Î∂ÑÏÑù Í≤∞Í≥º Í∏∏Ïù¥:', imageAnalysis.length, 'Ïûê');

    // ‚ú® STEP 2: ÏõêÎ≥∏ ÎπÑÏú® Í≥ÑÏÇ∞ (auto Ï≤òÎ¶¨ Î∞©Ïãù)
    const originalDimensions = await getImageDimensions(imageBase64);
    const inputRatio = originalDimensions.width / originalDimensions.height;
    
    let targetSize;
    if (inputRatio === 1) {
      targetSize = '1024x1024';  // Ï†ïÏÇ¨Í∞ÅÌòï
    } else if (inputRatio > 1.5) {
      targetSize = '1792x1024';  // 16:9 Ïù¥ÏÉÅ ‚Üí Í∞ÄÎ°úÌòï
    } else if (inputRatio < 0.7) {
      targetSize = '1024x1792';  // 9:16 Ïù¥Ìïò ‚Üí ÏÑ∏Î°úÌòï  
    } else {
      targetSize = '1024x1024';  // Í∑∏ Ïô∏ ‚Üí Ï†ïÏÇ¨Í∞ÅÌòïÏúºÎ°ú Ï≤òÎ¶¨
    }
    
    console.log('[gpt-image-1] üìê ÎπÑÏú® Î∂ÑÏÑù:', {
      original: `${originalDimensions.width}x${originalDimensions.height}`,
      ratio: inputRatio.toFixed(2),
      targetSize: targetSize
    });

    // ‚ú® STEP 3: gpt-image-1 Ï†ÑÏö© ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± (ÌôïÏÇ∞ Í∏∞Î∞ò)
    const gptImage1Prompt = `
TECHNICAL RECONSTRUCTION PROMPT:

Based on detailed analysis: "${imageAnalysis}"

FACE TRANSFORMATION TARGET: ${prompt}

PROCESSING WEIGHTS:
- face_change_weight: 1.0 (MAXIMUM transformation)
- hair_preservation_weight: 0.95 (Nearly perfect preservation)
- background_freeze: true (100% preservation)

DIFFUSION PARAMETERS:
- Replace ONLY facial features with target specifications
- Maintain all non-facial elements with high fidelity
- Use attention mechanism with higher weight on hair regions
- Preserve original lighting and atmospheric conditions
- Keep exact pose and body positioning

QUALITY SETTINGS:
- Photorealistic rendering required
- Professional portrait quality
- Natural skin texture generation
- Seamless feature integration

Execute face transformation while preserving all analyzed elements.
    `.trim();

    // ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥ Ï†úÌïú
    const finalPrompt = gptImage1Prompt.length > 4000 
      ? gptImage1Prompt.substring(0, 3997) + '...'
      : gptImage1Prompt;

    console.log('[gpt-image-1] üéØ Step 2: ÌôïÏÇ∞ Í∏∞Î∞ò Ïû¨Íµ¨ÏÑ± ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±');
    console.log('[gpt-image-1] üìù ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥:', finalPrompt.length, 'Ïûê');

    // ‚ú® STEP 4: DALL-E 3Î°ú ÌôïÏÇ∞ Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ Ïû¨Íµ¨ÏÑ±
    console.log('[gpt-image-1] üé® Step 3: DALL-E 3 ÌôïÏÇ∞ Í∏∞Î∞ò Ïû¨Íµ¨ÏÑ±');
    
    const generationResponse = await fetch('https://api.openai.com/v1/images/generations', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'dall-e-3',
        prompt: finalPrompt,
        n: 1,
        size: targetSize,  // ÏõêÎ≥∏ ÎπÑÏú® Ïú†ÏßÄ
        quality: 'hd',
        style: 'natural',  // ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïä§ÌÉÄÏùº
        response_format: 'b64_json'
      })
    });

    if (!generationResponse.ok) {
      const errorData = await generationResponse.json();
      console.error('[gpt-image-1] ‚ùå DALL-E 3 Ïã§Ìå®:', errorData);
      
      // Ìè¥Î∞±: DALL-E 2 Edit Î∞©Ïãù
      console.log('[gpt-image-1] üîÑ Ìè¥Î∞±: DALL-E 2 Edit Î∞©Ïãù');
      return await fallbackToEdit(apiKey, imageBase64, prompt, corsHeaders);
    }

    const generationData = await generationResponse.json();
    
    console.log('[gpt-image-1] ‚úÖ DALL-E 3 Ïû¨Íµ¨ÏÑ± ÏôÑÎ£å');
    
    if (generationData.data && generationData.data[0] && generationData.data[0].b64_json) {
      const resultBase64 = generationData.data[0].b64_json;
      
      // ‚ú® STEP 5: Í≤∞Í≥º Í≤ÄÏ¶ù (Î≥ÄÌôò Ï†ïÎèÑ ÌôïÏù∏)
      const verificationResult = await verifyTransformation(imageBase64, resultBase64);
      
      console.log('[gpt-image-1] üî¨ Î≥ÄÌôò Í≤ÄÏ¶ù:', verificationResult);
      
      console.log('[gpt-image-1] üéâ gpt-image-1 ÌîÑÎ°úÏÑ∏Ïä§ ÏôÑÎ£å!');
      
      return {
        statusCode: 200,
        headers: corsHeaders,
        body: JSON.stringify({
          data: [{
            b64_json: resultBase64
          }],
          model: 'gpt-image-1',
          processing_method: 'GPT-4V_Analysis + DALL-E-3_Reconstruction',
          verification: verificationResult
        })
      };
    } else {
      throw new Error('No image data in DALL-E 3 response');
    }
    
  } catch (error) {
    console.error('[gpt-image-1] üí• Ïò§Î•ò:', error.message);
    
    return {
      statusCode: 500,
      headers: corsHeaders,
      body: JSON.stringify({ 
        error: error.message,
        useGeminiFallback: true
      })
    };
  }
};

// üîß Î≥¥Ï°∞ Ìï®ÏàòÎì§

// Ïù¥ÎØ∏ÏßÄ Ï∞®Ïõê Ï∂îÏ∂ú
async function getImageDimensions(base64) {
  return new Promise((resolve) => {
    const img = new Image();
    img.onload = () => {
      resolve({ width: img.width, height: img.height });
    };
    img.src = `data:image/png;base64,${base64}`;
  });
}

// Î≥ÄÌôò Í≤∞Í≥º Í≤ÄÏ¶ù
async function verifyTransformation(originalBase64, resultBase64) {
  const originalSize = originalBase64.length;
  const resultSize = resultBase64.length;
  const sizeDiff = Math.abs(resultSize - originalSize);
  const changePercent = ((sizeDiff / originalSize) * 100).toFixed(1);
  
  return {
    sizeDifference: Math.round(sizeDiff / 1024) + 'KB',
    changePercent: changePercent + '%',
    transformationDetected: sizeDiff > 5000,
    confidence: sizeDiff > 10000 ? 'high' : sizeDiff > 5000 ? 'medium' : 'low'
  };
}

// Edit API Ìè¥Î∞±
async function fallbackToEdit(apiKey, imageBase64, prompt, corsHeaders) {
  console.log('[gpt-image-1] üîÑ Edit API Ìè¥Î∞± Ïã§Ìñâ');
  
  const boundary = '----fallback-edit-' + Math.random().toString(36).substring(2, 15);
  const formData = createEditFormData(boundary, imageBase64, prompt);
  
  const editResponse = await fetch('https://api.openai.com/v1/images/edits', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': `multipart/form-data; boundary=${boundary}`
    },
    body: formData
  });

  const editData = await editResponse.json();
  
  return {
    statusCode: editResponse.status,
    headers: corsHeaders,
    body: JSON.stringify({
      ...editData,
      model: 'gpt-image-1-fallback',
      processing_method: 'DALL-E-2_Edit_Fallback'
    })
  };
}

// Edit FormData ÏÉùÏÑ±
function createEditFormData(boundary, imageBase64, prompt) {
  const imageBuffer = Buffer.from(imageBase64, 'base64');
  const editPrompt = `Transform face to: ${prompt}. Keep hair, background, pose identical.`;
  
  const formParts = [
    `--${boundary}`,
    'Content-Disposition: form-data; name="model"',
    '',
    'dall-e-2',
    `--${boundary}`,
    'Content-Disposition: form-data; name="prompt"',
    '',
    editPrompt,
    `--${boundary}`,
    'Content-Disposition: form-data; name="size"',
    '',
    '1024x1024',
    `--${boundary}`,
    'Content-Disposition: form-data; name="n"',
    '',
    '1',
    `--${boundary}`,
    'Content-Disposition: form-data; name="response_format"',
    '',
    'b64_json',
    `--${boundary}`,
    'Content-Disposition: form-data; name="image"; filename="input.png"',
    'Content-Type: image/png',
    ''
  ];
  
  const textPart = formParts.join('\r\n') + '\r\n';
  const closingBoundary = `\r\n--${boundary}--\r\n`;
  
  return Buffer.concat([
    Buffer.from(textPart, 'utf8'),
    imageBuffer,
    Buffer.from(closingBoundary, 'utf8')
  ]);
}
